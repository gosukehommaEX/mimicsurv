---
title: "Getting Started with mimicsurv"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with mimicsurv}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(mimicsurv)
```

## Introduction

The `mimicsurv` package provides tools for survival analysis from published Kaplan-Meier tables using the person-years method. This vignette demonstrates the basic usage of the package functions with a comprehensive theoretical foundation.

## Basic Survival Analysis

### Extracting Results from Kaplan-Meier Tables

The main function `extractfromKM()` estimates hazard rates and median survival time from published Kaplan-Meier data:

```{r basic-analysis}
# Example data from a hypothetical study
time_points <- c(0, 6, 12, 18, 24)
n_risk <- c(200, 150, 100, 60, 35)
n_censored <- c(0, 10, 20, 30, 40)

# Extract survival analysis results
result <- extractfromKM(time_points, n_risk, n_censored)

# View hazard table
print(result$hazard_table)

# View median survival time
cat("Median survival time:", result$median_survival, "months\n")
```

### Understanding the Results

The hazard table shows:

-   **interval**: Time intervals for hazard estimation
-   **n_at_risk_start**: Number of patients at risk at the start of each interval
-   **n_censored_interval**: Number of patients censored within each interval
-   **n_events**: Number of events (estimated) within each interval
-   **hazard_rate**: Estimated hazard rate (events per person-time)

## Mathematical Background

The `mimicsurv` package implements the person-years method for hazard estimation based on several key epidemiological and statistical principles. Understanding these foundations is crucial for proper interpretation of results.

### Fundamental Assumptions

The method relies on the following critical assumptions:

#### 1. **Piecewise Exponential Survival**

Within each time interval $[t_i, t_{i+1})$, the hazard rate $\lambda_i$ is constant. This implies that the survival function within interval $i$ follows: $$S(t | t \in [t_i, t_{i+1})) = S(t_i) \exp(-\lambda_i (t - t_i))$$

#### 2. **Non-informative Censoring**

Censoring is independent of the event process. Formally, if $T$ is the event time and $C$ is the censoring time, then $T \perp C$. This ensures that censored observations provide unbiased information about the risk set.

#### 3. **Uniform Distribution of Events and Censoring Within Intervals**

**This is the key assumption for person-time calculation.** We assume that both events and censoring occur uniformly (with equal probability) at any point within each interval.

Mathematically, if an event occurs in interval $[t_i, t_{i+1})$, its time $T$ follows: $$T | T \in [t_i, t_{i+1}) \sim \text{Uniform}(t_i, t_{i+1})$$

**Consequence:** Under this assumption, the expected observation time for an individual experiencing an event or censoring within the interval is: $$E[T - t_i | T \in [t_i, t_{i+1})] = \frac{t_{i+1} - t_i}{2} = \frac{\Delta t}{2}$$

#### 4. **Administrative Censoring at Interval Boundaries**

Individuals who survive the entire interval without events or censoring contribute the full interval length to observation time.

### Person-Time Calculation: Theoretical Foundation

Person-time represents the total observation time contributed by all individuals. The calculation accounts for different contribution patterns:

#### **Complete Interval Contributors**

Individuals who neither experience events nor are censored within interval $[t_i, t_{i+1})$ contribute: - **Count**: $n_{\text{complete}} = n_{\text{risk},i+1}$ - **Time per person**: $\Delta t = t_{i+1} - t_i$ - **Total contribution**: $n_{\text{complete}} \times \Delta t$

#### **Partial Interval Contributors**

Individuals experiencing events or censoring within the interval contribute: - **Count**: $n_{\text{partial}} = n_{\text{events}} + n_{\text{censored}} = n_{\text{risk},i} - n_{\text{risk},i+1}$ - **Expected time per person**: $\frac{\Delta t}{2}$ (due to uniform distribution assumption) - **Total contribution**: $n_{\text{partial}} \times \frac{\Delta t}{2}$

#### **Mathematical Justification for** $\Delta t / 2$

Under the uniform distribution assumption, if $U \sim \text{Uniform}(0, \Delta t)$ represents the time from interval start to event/censoring, then: $$E[U] = \frac{0 + \Delta t}{2} = \frac{\Delta t}{2}$$

This is why we multiply by $\frac{\Delta t}{2}$ rather than some other fraction.

### Complete Person-Time Formula

Combining both contributor types:

$$\text{Person-time}_i = n_{\text{complete}} \times \Delta t + n_{\text{partial}} \times \frac{\Delta t}{2}$$

Substituting the definitions: $$\text{Person-time}_i = n_{\text{risk},i+1} \times \Delta t + (n_{\text{risk},i} - n_{\text{risk},i+1}) \times \frac{\Delta t}{2}$$

### Equivalence to Trapezoidal Rule

The above formula is mathematically equivalent to the commonly used trapezoidal rule.

**Algebraic proof:** $$n_{\text{risk},i+1} \times \Delta t + (n_{\text{risk},i} - n_{\text{risk},i+1}) \times \frac{\Delta t}{2}$$

$$= n_{\text{risk},i+1} \times \Delta t + n_{\text{risk},i} \times \frac{\Delta t}{2} - n_{\text{risk},i+1} \times \frac{\Delta t}{2}$$

$$= n_{\text{risk},i+1} \times \frac{\Delta t}{2} + n_{\text{risk},i} \times \frac{\Delta t}{2}$$

$$= \frac{n_{\text{risk},i} + n_{\text{risk},i+1}}{2} \times \Delta t$$

**Important insight:** The trapezoidal rule is not just a mathematical convenience—it has a solid epidemiological foundation based on the uniform distribution assumption.

### When the Uniform Distribution Assumption May Fail

The uniform distribution assumption may be violated when:

1.  **Systematic bias in event timing**: Events tend to cluster at specific times within intervals
2.  **Informative censoring patterns**: Censoring becomes more likely as events become more likely
3.  **Very long intervals**: The constant hazard assumption becomes less plausible over extended periods

In such cases, alternative approaches (e.g., shorter intervals, different distributional assumptions) may be necessary.

### Hazard Rate Estimation

Under the piecewise exponential assumption, the hazard rate for interval $i$ is estimated as:

$$\hat{\lambda}_i = \frac{\text{Number of events}_i}{\text{Person-time}_i}$$

**Statistical Properties:** - This estimator is the **maximum likelihood estimator** under the exponential distribution assumption - It is **unbiased**: $E[\hat{\lambda}_i] = \lambda_i$ - For large samples, it follows approximately: $\hat{\lambda}_i \sim N(\lambda_i, \lambda_i/\text{Person-time}_i)$

### Median Survival Time

For piecewise exponential survival, the survival function is:

$$S(t) = \exp\left(-\sum_{j=1}^{k} \lambda_j \times \min(\Delta t_j, \max(0, t - t_{j-1}))\right)$$

The median survival time $t_{0.5}$ satisfies $S(t_{0.5}) = 0.5$. For interval $k$ containing the median:

$$t_{\text{median}} = t_k + \frac{1}{\lambda_k} \ln\left(\frac{S(t_k)}{0.5}\right)$$

**Note:** If $S(t_{\text{final}}) \geq 0.5$, the median is not reached within the observation period.

### Practical Example

Using our study data, let's trace through the calculations:

```{r mathematical-example}
# Our study data
time_points <- c(0, 6, 12, 18, 24)
n_risk <- c(200, 150, 100, 60, 35)
n_censored <- c(0, 10, 20, 30, 40)

# Calculate for first interval [0,6)
interval_1 <- list(
  n_at_start = 200,
  n_at_end = 150,
  n_censored_in_interval = 10 - 0,  # 10
  n_events_in_interval = 200 - 150 - 10,  # 40
  interval_length = 6 - 0  # 6
)

# Person-time calculation
person_time_1 <- interval_1$n_at_end * interval_1$interval_length + 
                 (interval_1$n_events_in_interval + interval_1$n_censored_in_interval) * 
                 interval_1$interval_length / 2

cat("Interval [0,6):\n")
cat("- Complete observation:", interval_1$n_at_end, "persons ×", interval_1$interval_length, "months =", 
    interval_1$n_at_end * interval_1$interval_length, "person-months\n")
cat("- Partial observation:", interval_1$n_events_in_interval + interval_1$n_censored_in_interval, 
    "persons ×", interval_1$interval_length/2, "months =", 
    (interval_1$n_events_in_interval + interval_1$n_censored_in_interval) * interval_1$interval_length/2, "person-months\n")
cat("- Total person-time:", person_time_1, "person-months\n")

# Hazard rate calculation
hazard_rate_1 <- interval_1$n_events_in_interval / person_time_1
cat("- Hazard rate:", interval_1$n_events_in_interval, "events /", person_time_1, 
    "person-months =", round(hazard_rate_1, 6), "per month\n")
```

### Assumptions in Practice: Critical Considerations

#### **Uniform Distribution Assumption**

This is often the most questionable assumption. In reality: - **Early events**: Some conditions have higher risk immediately after exposure - **Late events**: Progressive diseases may show increasing risk over time - **Periodic patterns**: Some events follow circadian or seasonal patterns

**Sensitivity analysis**: Consider varying interval lengths or using alternative assumptions when this seems implausible.

#### **Constant Hazard Within Intervals**

This assumption is more reasonable when: - Intervals are relatively short (e.g., 3-6 months for chronic diseases) - The underlying biological process has constant risk - No major time-varying confounders within intervals

#### **Non-informative Censoring**

This may be violated when: - Patients withdraw due to disease progression - Loss to follow-up is related to prognosis - Administrative censoring correlates with risk factors

## Simulation and Validation

### Generating Simulated Data

Now let's work backwards - starting from our estimated hazard rates, we can simulate data to validate our method:

```{r simulation}
# Use the hazard rates estimated from our KM table
estimated_hazards <- result$hazard_table$hazard_rate
cat("Estimated hazard rates:", paste(round(estimated_hazards, 4), collapse = ", "), "\n")

# Simulate data using these estimated parameters
set.seed(123)  # For reproducible results
sim_data <- simPE(
  n = 200,  # Same initial sample size as our study
  time_points = time_points,
  hazard_rates = estimated_hazards,
  max_time = 24,
  censoring_prob = 0.1
)

# Check the simulated data
cat("Simulated data summary:\n")
cat("- Total subjects:", nrow(sim_data), "\n")
cat("- Events:", sum(sim_data$status == 1), "\n") 
cat("- Censored:", sum(sim_data$status == 0), "\n")
cat("- Event rate:", round(mean(sim_data$status), 2), "\n")
```

### Creating Summary Tables

Let's create a KM-style summary from our simulated data and compare it to the original:

```{r summary-table}
# Create KM table from simulated data
km_summary <- summaryKM(sim_data, time_points)

cat("Comparison of original vs simulated data:\n")
comparison_table <- data.frame(
  Time = time_points,
  Original_N_Risk = c(200, 150, 100, 60, 35),
  Simulated_N_Risk = km_summary$n_risk,
  Original_N_Censored = c(0, 10, 20, 30, 40),
  Simulated_N_Censored = km_summary$n_censored_cumulative
)
print(comparison_table)

# Re-estimate hazard rates from simulated data
sim_result <- extractfromKM(
  time_points = time_points,
  n_risk = km_summary$n_risk,
  n_censored = km_summary$n_censored_cumulative
)

cat("\nHazard rate comparison:\n")
hazard_comparison <- data.frame(
  Interval = result$hazard_table$interval,
  Original_Hazard = result$hazard_table$hazard_rate,
  Simulated_Hazard = sim_result$hazard_table$hazard_rate,
  Relative_Error = abs(sim_result$hazard_table$hazard_rate - result$hazard_table$hazard_rate) / 
                   result$hazard_table$hazard_rate * 100
)
print(hazard_comparison)
```

## Validation Study

### Comprehensive Validation

The `validate_mimicsurv()` function provides comprehensive validation of the hazard estimation methods:

```{r validation, eval=FALSE}
# Run validation study (small example for demonstration)
validation_result <- validate_mimicsurv(
  true_time_points = time_points,
  true_hazard_rates = estimated_hazards,
  sample_sizes = c(100, 200),
  n_simulations = 50,
  censoring_prob = 0.15
)

# View summary statistics
print(validation_result$summary_statistics)

# View detailed results for one sample size
print(validation_result$detailed_results$n_200$hazard_results)
```

### Method Validation Example

Here's a practical example showing how well our method recovers known parameters:

```{r practical-validation}
# Set known "true" parameters similar to our study
true_times <- c(0, 6, 12, 18, 24)
true_hazards <- c(0.038, 0.053, 0.063, 0.053)  # Similar to our estimated rates

# Calculate true median survival
true_median <- getMediansurv(true_times, true_hazards)
cat("True median survival:", round(true_median, 1), "months\n")

# Simulate larger dataset for better precision
set.seed(456)
large_sim_data <- simPE(
  n = 1000,  # Larger sample for better precision
  time_points = true_times,
  hazard_rates = true_hazards,
  max_time = 24,
  censoring_prob = 0.15
)

# Create KM table from simulated data
large_km_table <- summaryKM(large_sim_data, true_times)

# Estimate parameters from the KM table
estimated_result <- extractfromKM(
  time_points = true_times,
  n_risk = large_km_table$n_risk,
  n_censored = large_km_table$n_censored_cumulative
)

# Compare results
cat("\nValidation Results (n=1000):\n")
validation_comparison <- data.frame(
  Interval = estimated_result$hazard_table$interval,
  True_Hazard = true_hazards,
  Estimated_Hazard = estimated_result$hazard_table$hazard_rate,
  Relative_Error = abs(estimated_result$hazard_table$hazard_rate - true_hazards) / true_hazards * 100
)
print(validation_comparison)

cat("\nMedian Survival Comparison:\n")
cat("True median:", round(true_median, 1), "months\n")
cat("Estimated median:", round(estimated_result$median_survival, 1), "months\n")
if (!is.na(estimated_result$median_survival)) {
  relative_error <- abs(estimated_result$median_survival - true_median) / true_median * 100
  cat("Relative error:", round(relative_error, 1), "%\n")
}
```

## Advanced Usage

### Calculating True Median Survival

You can calculate the theoretical median survival time directly:

```{r true-median}
# Calculate true median for comparison using our estimated hazards
true_median <- getMediansurv(time_points, estimated_hazards)
cat("Theoretical median survival:", round(true_median, 2), "months\n")
```

### Handling Different Scenarios

The package handles various scenarios:

-   **High censoring rates**: Appropriate for studies with substantial loss to follow-up
-   **Multiple intervals**: Flexible number of time intervals
-   **Missing median**: Returns NA when median survival is not reached
-   **Edge cases**: Robust handling of boundary conditions

## Best Practices

1.  **Data Quality**: Ensure input data is consistent (number at risk should decrease over time)
2.  **Interval Selection**: Use meaningful time intervals that reflect clinical practice
3.  **Assumption Checking**: Verify that the uniform distribution assumption is reasonable
4.  **Validation**: Use simulation studies to validate results when possible
5.  **Interpretation**: Consider the assumptions of piecewise exponential survival
6.  **Sensitivity Analysis**: Test robustness to different interval definitions

## Conclusion

The `mimicsurv` package provides a comprehensive framework for extracting survival information from published Kaplan-Meier data. The combination of:

-   **Rigorous theoretical foundation** with clear assumptions
-   **Robust estimation methods** based on maximum likelihood principles
-   **Comprehensive validation tools** for method verification
-   **Practical examples** demonstrating real-world applications

makes it a valuable tool for meta-analyses and secondary data analyses in survival research. The person-years method, while based on specific assumptions, provides a statistically sound approach to extracting quantitative information from published survival curves when individual patient data are not available.
